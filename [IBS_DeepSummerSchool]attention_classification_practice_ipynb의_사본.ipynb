{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[IBS_DeepSummerSchool]attention_classification_practice.ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kafkapple/DDSA/blob/master/%5BIBS_DeepSummerSchool%5Dattention_classification_practice_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ZxV4BJvN2y",
        "colab_type": "text"
      },
      "source": [
        "# Load utility codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLaD4ba2vKFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "def split_dataset(x_test, y_test, dev_ratio):\n",
        "    \"\"\"split test dataset to test and dev set with ratio \"\"\"\n",
        "    test_size = len(x_test)\n",
        "    print(test_size)\n",
        "    dev_size = (int)(test_size * dev_ratio)\n",
        "    print(dev_size)\n",
        "    x_dev = x_test[:dev_size]\n",
        "    x_test = x_test[dev_size:]\n",
        "    y_dev = y_test[:dev_size]\n",
        "    y_test = y_test[dev_size:]\n",
        "    return x_test, x_dev, y_test, y_dev, dev_size, test_size - dev_size\n",
        "\n",
        "\n",
        "def fill_feed_dict(data_X, data_Y, batch_size):\n",
        "    \"\"\"Generator to yield batches\"\"\"\n",
        "    # Shuffle data first.\n",
        "    shuffled_X, shuffled_Y = shuffle(data_X, data_Y)\n",
        "    # print(\"before shuffle: \", data_Y[:10])\n",
        "    # print(data_X.shape[0])\n",
        "    # perm = np.random.permutation(data_X.shape[0])\n",
        "    # data_X = data_X[perm]\n",
        "    # shuffled_Y = data_Y[perm]\n",
        "    # print(\"after shuffle: \", shuffled_Y[:10])\n",
        "    for idx in range(data_X.shape[0] // batch_size):\n",
        "        x_batch = shuffled_X[batch_size * idx: batch_size * (idx + 1)]\n",
        "        y_batch = shuffled_Y[batch_size * idx: batch_size * (idx + 1)]\n",
        "        \n",
        "        yield (x_batch, y_batch)\n",
        "        \n",
        "        \n",
        "def make_train_feed_dict(model, batch):\n",
        "    \"\"\"make train feed dict for training\"\"\"\n",
        "    feed_dict = {model.x: batch[0],\n",
        "                 model.label: batch[1],\n",
        "                 model.keep_prob: .5}\n",
        "    return feed_dict\n",
        "\n",
        "\n",
        "def make_test_feed_dict(model, batch):\n",
        "    feed_dict = {model.x: batch[0],\n",
        "                 model.label: batch[1],\n",
        "                 model.keep_prob: 1.0}\n",
        "    return feed_dict\n",
        "\n",
        "\n",
        "def run_train_step(model, sess, batch):\n",
        "    feed_dict = make_train_feed_dict(model, batch)\n",
        "    to_return = {\n",
        "        'train_op': model.train_op,\n",
        "        'loss': model.loss,\n",
        "        'global_step': model.global_step,\n",
        "    }\n",
        "    return sess.run(to_return, feed_dict)\n",
        "\n",
        "\n",
        "def run_eval_step(model, sess, batch):\n",
        "    feed_dict = make_test_feed_dict(model, batch)\n",
        "    prediction = sess.run(model.prediction, feed_dict)\n",
        "    acc = np.sum(np.equal(prediction, batch[1])) / len(prediction)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def get_attn_weight(model, sess, batch):\n",
        "    feed_dict = make_train_feed_dict(model, batch)\n",
        "    return sess.run(model.alpha, feed_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIQpQCFSvXoX",
        "colab_type": "text"
      },
      "source": [
        "# Define Attention-based LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkjX0XI4u0xA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
        "from tensorflow.contrib.rnn import BasicLSTMCell\n",
        "import time\n",
        "\n",
        "\n",
        "class ABLSTM(object):\n",
        "    def __init__(self, config):\n",
        "        self.max_len = config[\"max_len\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "        self.n_class = config[\"n_class\"]\n",
        "        self.learning_rate = config[\"learning_rate\"]\n",
        "\n",
        "        # placeholder\n",
        "        self.x = tf.placeholder(tf.int32, [None, self.max_len])\n",
        "        self.label = tf.placeholder(tf.int32, [None])\n",
        "        self.keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "    def build_graph(self):\n",
        "        print(\"building graph\")\n",
        "        # Word embedding\n",
        "        embeddings_var = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n",
        "                                     trainable=True)\n",
        "        batch_embedded = tf.nn.embedding_lookup(embeddings_var, self.x)\n",
        "\n",
        "        # take embedded inputs as input of bi-directional rnn\n",
        "        rnn_outputs, _ = bi_rnn(BasicLSTMCell(self.hidden_size),\n",
        "                                BasicLSTMCell(self.hidden_size),\n",
        "                                inputs=batch_embedded, dtype=tf.float32)\n",
        "        fw_outputs, bw_outputs = rnn_outputs        \n",
        "        \n",
        "        # combine biRNN outputs using element-wise sum\n",
        "        H = fw_outputs + bw_outputs  # (batch_size, max_len, hidden_size)\n",
        "        \n",
        "        # Alignment model\n",
        "        M = tf.tanh(H)  # (batch_size, max_len, hidden_size)\n",
        "        M = tf.reshape(M, [-1, self.hidden_size])  # (batch_size x max_len, hidden_size)\n",
        "        W = tf.Variable(tf.random_normal([self.hidden_size, 1], stddev=0.1))  # (hidden_size, 1)\n",
        "        \n",
        "        MW = tf.matmul(M, W)  # (batch_size x max_len, 1)\n",
        "        MW = tf.reshape(MW, (-1, self.max_len))  # (batch_size, max_len)\n",
        "        \n",
        "        # Attention weights over paragraphs\n",
        "        self.alpha = tf.nn.softmax(MW)  # (batch_size, max_len)\n",
        "        r = tf.matmul(tf.transpose(H, [0, 2, 1]),  # (batch_size, hidden_size, max_len)\n",
        "                      tf.reshape(self.alpha, [-1, self.max_len, 1]))  # (batch_size, max_len, 1)\n",
        "        # r - (batch_size, hidden_size, 1)\n",
        "        \n",
        "        r = tf.squeeze(r)  # (batch_size, hidden_size)\n",
        "        h_star = tf.tanh(r)  # (batch_size, hidden_size)\n",
        "\n",
        "        h_drop = tf.nn.dropout(h_star, self.keep_prob)\n",
        "\n",
        "        # Fully connected layer（dense layer)\n",
        "        FC_W = tf.Variable(tf.truncated_normal([self.hidden_size, self.n_class], stddev=0.1))\n",
        "        FC_b = tf.Variable(tf.constant(0., shape=[self.n_class]))\n",
        "        y_hat = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
        "\n",
        "        self.loss = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_hat, labels=self.label))\n",
        "\n",
        "        # prediction\n",
        "        self.prediction = tf.argmax(tf.nn.softmax(y_hat), 1)\n",
        "\n",
        "        # optimization\n",
        "        loss_to_minimize = self.loss\n",
        "        tvars = tf.trainable_variables()\n",
        "        gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
        "        grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n",
        "\n",
        "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "        self.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step,\n",
        "                                                       name='train_step')\n",
        "        print(\"graph built successfully!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihkQlI560pSQ",
        "colab_type": "text"
      },
      "source": [
        "# Main code\n",
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXyKeAhIxeLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imdb = tf.keras.datasets.imdb\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYlbvNZf0wpc",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS37swlOu5r0",
        "colab_type": "code",
        "outputId": "b312898b-1348-4acf-fa88-09ab1e99d5f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "config = {\n",
        "    \"max_len\": 256,\n",
        "    \"hidden_size\": 64,\n",
        "    \"vocab_size\": 10004,\n",
        "    \"embedding_size\": 128,\n",
        "    \"n_class\": 2,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": 32,\n",
        "    \"train_epoch\": 5\n",
        "}\n",
        "\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=config[\"max_len\"])\n",
        "\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=config[\"max_len\"])\n",
        "\n",
        "# split dataset to test and dev\n",
        "x_test, x_dev, y_test, y_dev, dev_size, test_size = split_dataset(x_test, y_test, 0.1)\n",
        "print(\"Validation Size: \", dev_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "2500\n",
            "Validation Size:  2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjjHzwJ7rri2",
        "colab_type": "text"
      },
      "source": [
        "## Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_ntFoOA0-VC",
        "colab_type": "code",
        "outputId": "d6e97fe8-3112-48db-cee9-a0b463b11ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "classifier = ABLSTM(config)\n",
        "classifier.build_graph()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "dev_batch = (x_dev, y_dev)\n",
        "start = time.time()\n",
        "for e in range(config[\"train_epoch\"]):\n",
        "\n",
        "    t0 = time.time()\n",
        "    print(\"Epoch %d start !\" % (e + 1))\n",
        "    for x_batch, y_batch in fill_feed_dict(x_train, y_train, config[\"batch_size\"]):\n",
        "        return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n",
        "        # attn = get_attn_weight(classifier, sess, (x_batch, y_batch))\n",
        "        # plot the attention weight\n",
        "        # print(np.reshape(attn, (config[\"batch_size\"], config[\"max_len\"])))\n",
        "    t1 = time.time()\n",
        "\n",
        "    print(\"Train Epoch time:  %.3f s\" % (t1 - t0))\n",
        "    dev_acc = run_eval_step(classifier, sess, dev_batch)\n",
        "    print(\"validation accuracy: %.3f \" % dev_acc)\n",
        "\n",
        "print(\"Training finished, time consumed : \", time.time() - start, \" s\")\n",
        "print(\"Start evaluating:  \\n\")\n",
        "cnt = 0\n",
        "test_acc = 0\n",
        "for x_batch, y_batch in fill_feed_dict(x_test, y_test, config[\"batch_size\"]):\n",
        "    acc = run_eval_step(classifier, sess, (x_batch, y_batch))\n",
        "    test_acc += acc\n",
        "    cnt += 1\n",
        "\n",
        "print(\"Test accuracy : %f %%\" % (test_acc / cnt * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0623 14:24:34.975967 140042821113728 deprecation.py:323] From <ipython-input-6-906a954c606c>:28: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0623 14:24:34.978104 140042821113728 deprecation.py:323] From <ipython-input-6-906a954c606c>:30: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "W0623 14:24:34.979575 140042821113728 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0623 14:24:35.070816 140042821113728 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0623 14:24:35.085621 140042821113728 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building graph\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0623 14:24:35.817292 140042821113728 deprecation.py:506] From <ipython-input-6-906a954c606c>:53: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0623 14:24:36.414523 140042821113728 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "graph built successfully!\n",
            "Epoch 1 start !\n",
            "Train Epoch time:  231.514 s\n",
            "validation accuracy: 0.840 \n",
            "Epoch 2 start !\n",
            "Train Epoch time:  231.022 s\n",
            "validation accuracy: 0.882 \n",
            "Epoch 3 start !\n",
            "Train Epoch time:  230.521 s\n",
            "validation accuracy: 0.876 \n",
            "Epoch 4 start !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osMEGUSDwF89",
        "colab_type": "text"
      },
      "source": [
        "# Reference \n",
        " + https://github.com/TobiasLee/Text-Classification\n",
        " + https://www.tensorflow.org/tutorials/keras/basic_text_classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk7opM0b4j4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}